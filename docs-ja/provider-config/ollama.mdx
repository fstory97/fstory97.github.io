---
title: "Ollama"
---

Cline は Ollama を使用してモデルをローカルで実行することをサポートしています。このアプローチはプライバシー、オフラインアクセス、潜在的なコスト削減を提供します。初期設定と十分に強力なコンピューターが必要です。現在の消費者向けハードウェアの状況により、平均的なハードウェア構成ではパフォーマンスが低下する可能性が高いため、Cline で Ollama を使用することは推奨されません。

**ウェブサイト:** [https://ollama.com/](https://ollama.com/)

### Ollama のセットアップ

1.  **Ollama のダウンロードとインストール:**
    [Ollama ウェブサイト](https://ollama.com/)からお使いのオペレーティングシステム用の Ollama インストーラーを入手し、インストールガイドに従ってください。Ollama が実行されていることを確認してください。通常は以下で開始できます：

    ```bash
    ollama serve
    ```

2.  **モデルのダウンロード:**
    Ollama は幅広い種類のモデルをサポートしています。利用可能なモデルのリストは [Ollama モデルライブラリ](https://ollama.com/library)で確認できます。コーディングタスクに推奨されるモデルには以下があります：

    -   `codellama:7b-code`（優れた小型の開始点）
    -   `codellama:13b-code`（より良い品質、より大きなサイズ）
    -   `codellama:34b-code`（さらに高い品質、非常に大きい）
    -   `qwen2.5-coder:32b`
    -   `mistralai/Mistral-7B-Instruct-v0.1`（堅実な汎用モデル）
    -   `deepseek-coder:6.7b-base`（コーディングに効果的）
    -   `llama3:8b-instruct-q5_1`（一般的なタスクに適している）

    モデルをダウンロードするには、ターミナルを開いて以下を実行します：

    ```bash
    ollama pull <model_name>
    ```

    例えば：

    ```bash
    ollama pull qwen2.5-coder:32b
    ```

3.  **モデルのコンテキストウィンドウの設定:**
    デフォルトでは、Ollama モデルは 2048 トークンのコンテキストウィンドウを使用することが多く、多くの Cline リクエストには不十分な場合があります。適切な結果を得るには最低 12,000 トークンが推奨され、32,000 トークンが理想的です。これを調整するには、モデルのパラメーターを変更し、新しいバージョンとして保存します。

    まず、モデルをロードします（`qwen2.5-coder:32b` を例として使用）：

    ```bash
    ollama run qwen2.5-coder:32b
    ```

    Ollama のインタラクティブセッション内でモデルがロードされたら、コンテキストサイズパラメーターを設定します：

    ```
    /set parameter num_ctx 32768
    ```

    次に、この設定されたモデルを新しい名前で保存します：

    ```
    /save your_custom_model_name
    ```

    （`your_custom_model_name` を選択した名前に置き換えてください。）

4.  **Cline の設定:**
    -   Cline サイドバーを開きます（通常は Cline アイコンで示されます）。
    -   設定歯車アイコン（⚙️）をクリックします。
    -   API Provider として "ollama" を選択します。
    -   前のステップで保存したモデル名を入力します（例：`your_custom_model_name`）。
    -   （オプション）Ollama が異なるマシンやポートで実行されている場合は、ベース URL を調整します。デフォルトは `http://localhost:11434` です。
    -   （オプション）Cline の詳細設定でモデルコンテキストサイズを設定します。これにより、Cline がカスタマイズされた Ollama モデルでコンテキストウィンドウを効果的に管理できます。

### ヒントと注意事項

-   **リソース要求:** 大規模言語モデルをローカルで実行することは、システムリソースに負荷をかける可能性があります。選択したモデルの要件をコンピューターが満たしていることを確認してください。
-   **モデル選択:** さまざまなモデルを試して、特定のタスクと設定に最適なものを見つけてください。
-   **オフライン機能:** モデルをダウンロードした後は、インターネット接続なしでもそのモデルで Cline を使用できます。
-   **トークン使用量追跡:** Cline は Ollama 経由でアクセスされるモデルのトークン使用量を追跡し、消費量を監視できます。
-   **Ollama 独自のドキュメント:** より詳細な情報については、公式 [Ollama ドキュメント](https://ollama.com/docs)を参照してください。

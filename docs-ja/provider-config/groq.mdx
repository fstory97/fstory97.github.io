---
title: "Groq"
description: "Groq の超高速推論を Cline で設定・使用する方法について説明します。Groq の専用 LPU アーキテクチャで OpenAI、Meta、DeepSeek などのモデルにアクセスできます。"
---

Groq は、トレーニングハードウェアからの流用ではなく、推論のために特別に構築されたカスタム LPU™ (Language Processing Unit) アーキテクチャを通じて超高速 AI 推論を提供します。Groq は OpenAI、Meta、DeepSeek、Moonshot AI などの様々なプロバイダーのオープンソースモデルをホストしています。

**ウェブサイト:** [https://groq.com/](https://groq.com/)

### API キーの取得

1.  **サインアップ/サインイン:** [Groq](https://groq.com/) にアクセスし、アカウントを作成するかサインインします。
2.  **コンソールに移動:** [Groq Console](https://console.groq.com/) にアクセスして、ダッシュボードにアクセスします。
3.  **キーを作成:** API Keys セクションに移動し、新しい API キーを作成します。キーに分かりやすい名前を付けてください（例："Cline"）。
4.  **キーをコピー:** API キーをすぐにコピーしてください。再表示することはできません。安全に保管してください。

### サポートされているモデル

Cline は以下の Groq モデルをサポートしています：

-   `llama-3.3-70b-versatile` (Meta) - 131K コンテキストでバランスの取れたパフォーマンス
-   `llama-3.1-8b-instant` (Meta) - 131K コンテキストで高速推論  
-   `openai/gpt-oss-120b` (OpenAI) - 131K コンテキストのフラッグシップモデル
-   `openai/gpt-oss-20b` (OpenAI) - 131K コンテキストのコンパクトモデル
-   `moonshotai/kimi-k2-instruct` (Moonshot AI) - プロンプトキャッシュ搭載の 1 トリリオンパラメーターモデル
-   `deepseek-r1-distill-llama-70b` (DeepSeek/Meta) - 推論最適化モデル
-   `qwen/qwen3-32b` (Alibaba Cloud) - Q&A タスク強化版
-   `meta-llama/llama-4-maverick-17b-128e-instruct` (Meta) - 最新の Llama 4 バリアント
-   `meta-llama/llama-4-scout-17b-16e-instruct` (Meta) - 最新の Llama 4 バリアント

### Cline での設定

1.  **Cline 設定を開く:** Cline パネルの設定アイコン（⚙️）をクリックします。
2.  **プロバイダーを選択:** "API Provider" ドロップダウンから "Groq" を選択します。
3.  **API キーを入力:** "Groq API Key" フィールドに Groq API キーを貼り付けます。
4.  **モデルを選択:** "Model" ドロップダウンから希望するモデルを選択します。

### Groq の速度革命

Groq の LPU アーキテクチャは、従来の GPU ベースの推論に対していくつかの主要な利点を提供します：

#### LPU アーキテクチャ
トレーニングワークロードから転用された GPU とは異なり、Groq の LPU は推論のために特別に設計されています。これにより、従来のシステムでレイテンシを生み出すアーキテクチャのボトルネックが排除されます。

#### 比類なき速度
- **サブミリ秒レイテンシ**：トラフィック、地域、ワークロードに関係なく一貫性を維持
- **静的スケジューリング**：事前計算された実行グラフによりランタイム調整の遅延を排除
- **テンソル並列処理**：高スループットバッチ処理ではなく、低レイテンシの単一レスポンス用に最適化

#### トレードオフなしの品質
- **TruePoint numerics**：精度に影響しない領域でのみ精度を削減
- **100 ビット中間累積**：ロスレス計算を保証
- **戦略的精度制御**：BF16 より 2-4 倍の高速化を実現しながら品質を維持

#### メモリアーキテクチャ
- **プライマリストレージとしての SRAM**（キャッシュではない）：チップ上に数百メガバイト
- **DRAM/HBM レイテンシを排除**：従来のアクセラレーターを悩ませる問題を解決
- **真のテンソル並列処理を実現**：複数のチップ間でレイヤーを分割

Groq の技術について詳しくは、[LPU アーキテクチャブログ記事](https://groq.com/blog/inside-the-lpu-deconstructing-groq-speed)をご覧ください。

### 特別機能

#### プロンプトキャッシュ
Kimi K2 モデルはプロンプトキャッシュをサポートしており、繰り返しプロンプトのコストとレイテンシを大幅に削減できます。

#### ビジョンサポート
選択されたモデルは画像入力とビジョン機能をサポートしています。具体的な機能については、Groq Console でモデルの詳細を確認してください。

#### 推論モデル
DeepSeek バリアントなど一部のモデルは、ステップバイステップの思考プロセスで強化された推論機能を提供します。

### ヒントと注意事項

-   **モデル選択:** 特定の使用例とパフォーマンス要件に基づいてモデルを選択してください。
-   **速度の利点:** Groq は高スループットのバッチ処理ではなく、単一リクエストのレイテンシで優れています。
-   **OSS モデルプロバイダー:** Groq は複数のプロバイダー（OpenAI、Meta、DeepSeek など）のオープンソースモデルを高速インフラでホストしています。
-   **コンテキストウィンドウ:** ほとんどのモデルは大きなコンテキストウィンドウ（最大 131K トークン）を提供し、大量のコードとコンテキストを含めることができます。
-   **価格:** Groq は速度の利点と競争力のある価格を提供しています。現在の料金については [Groq Pricing](https://groq.com/pricing) ページを確認してください。
-   **レート制限:** Groq には寛大なレート制限がありますが、利用段階に基づいた現在の制限についてはドキュメントを確認してください。
